{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "accelerator": "GPU",
        "colab": {
            "collapsed_sections": [],
            "name": "C3_W3_Lab_3_Conv1D.ipynb",
            "private_outputs": true,
            "provenance": []
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C3/W3/ungraded_labs/C3_W3_Lab_3_Conv1D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ],
            "metadata": {
                "azdata_cell_guid": "72635fb1-78ac-4598-9eb6-00c8fafff0cd"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Ungraded Lab: Using Convolutional Neural Networks\n",
                "\n",
                "In this lab, you will look at another way of building your text classification model and this will be with a convolution layer. As you learned in Course 2 of this specialization, convolutions extract features by applying filters to the input. Let's see how you can use that for text data in the next sections."
            ],
            "metadata": {
                "id": "rFiCyWQ-NC5D",
                "azdata_cell_guid": "141809fd-e58b-407d-b50b-f1cb9cfacc65"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Download and prepare the dataset"
            ],
            "metadata": {
                "id": "djvGxIRDHT5e",
                "azdata_cell_guid": "35a9e529-4bea-4e32-a1fc-831c6d91bd91"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import tensorflow_datasets as tfds\n",
                "\n",
                "# Download the subword encoded pretokenized dataset\n",
                "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)\n",
                "\n",
                "# Get the tokenizer\n",
                "tokenizer = info.features['text'].encoder"
            ],
            "metadata": {
                "id": "Y20Lud2ZMBhW",
                "azdata_cell_guid": "e5b6fd88-668a-4056-a7fc-2155de12aa80",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "BUFFER_SIZE = 10000\n",
                "BATCH_SIZE = 256\n",
                "\n",
                "# Get the train and test splits\n",
                "train_data, test_data = dataset['train'], dataset['test'], \n",
                "\n",
                "# Shuffle the training data\n",
                "train_dataset = train_data.shuffle(BUFFER_SIZE)\n",
                "\n",
                "# Batch and pad the datasets to the maximum length of the sequences\n",
                "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
                "test_dataset = test_data.padded_batch(BATCH_SIZE)\n"
            ],
            "metadata": {
                "id": "AW-4Vo4TMUHb",
                "azdata_cell_guid": "a7ff6e16-8a33-4ea8-9d2a-86cc26ec4c3a",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Build the Model\n",
                "\n",
                "In Course 2, you were using 2D convolution layers because you were applying it on images. For temporal data such as text sequences, you will use [Conv1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D) instead so the convolution will happen over a single dimension. You will also append a pooling layer to reduce the output of the convolution layer. For this lab, you will use [GlobalMaxPooling1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool1D) to get the max value across the time dimension. You can also use average pooling and you will do that in the next labs. See how these layers behave as standalone layers in the cell below."
            ],
            "metadata": {
                "id": "nfatNr6-IAcd",
                "azdata_cell_guid": "1c19da90-e699-4c9d-9955-a2d8f37ce5d0"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "\n",
                "# Hyperparameters\n",
                "batch_size = 1\n",
                "timesteps = 20\n",
                "features = 20\n",
                "filters = 128\n",
                "kernel_size = 5\n",
                "\n",
                "print(f'batch_size: {batch_size}')\n",
                "print(f'timesteps (sequence length): {timesteps}')\n",
                "print(f'features (embedding size): {features}')\n",
                "print(f'filters: {filters}')\n",
                "print(f'kernel_size: {kernel_size}')\n",
                "\n",
                "# Define array input with random values\n",
                "random_input = np.random.rand(batch_size,timesteps,features)\n",
                "print(f'shape of input array: {random_input.shape}')\n",
                "\n",
                "# Pass array to convolution layer and inspect output shape\n",
                "conv1d = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')\n",
                "# If kernel_size = 5, it will eliminate 5 - 1  = 4 elements from the input sequence.\n",
                "# 2 elements in the beginning and 2 elements in the end of the sequence will be lost.\n",
                "result = conv1d(random_input)\n",
                "print(f'shape of conv1d output: {result.shape}')\n",
                "\n",
                "# Pass array to max pooling layer and inspect output shape\n",
                "gmp = tf.keras.layers.GlobalMaxPooling1D()\n",
                "result = gmp(result)\n",
                "print(f'shape of global max pooling output: {result.shape}')"
            ],
            "metadata": {
                "id": "Ay87qbqwIJaV",
                "azdata_cell_guid": "8012f36b-7d6a-471e-be0c-2a3f7fd02eaf",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "You can build the model by simply appending the convolution and pooling layer after the embedding layer as shown below."
            ],
            "metadata": {
                "id": "lNNYF7tqO7it",
                "azdata_cell_guid": "1f62f750-ba7d-4d3a-9810-d03e23b0a181"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import tensorflow as tf\n",
                "\n",
                "# Hyperparameters\n",
                "embedding_dim = 64\n",
                "filters = 128\n",
                "kernel_size = 5\n",
                "dense_dim = 64\n",
                "\n",
                "# Build the model\n",
                "model = tf.keras.Sequential([\n",
                "    tf.keras.layers.Embedding(tokenizer.vocab_size, embedding_dim),\n",
                "    tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'),\n",
                "    # If kernel_size = 5, it will eliminate 5 - 1  = 4 elements from the input sequence.\n",
                "    # 2 elements in the beginning and 2 elements in the end of the sequence will be lost.\n",
                "    tf.keras.layers.GlobalMaxPooling1D(),\n",
                "    tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
                "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
                "])\n",
                "\n",
                "# Print the model summary\n",
                "model.summary()"
            ],
            "metadata": {
                "id": "jo1jjO3vn0jo",
                "azdata_cell_guid": "b2694f38-9b85-4e63-9825-9350736afe4c",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Set the training parameters\n",
                "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
            ],
            "metadata": {
                "id": "Uip7QOVzMoMq",
                "azdata_cell_guid": "850c18bd-5739-45fd-9d62-57bb62f096ab",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Train the model\n",
                "\n",
                "Training will take around 30 seconds per epoch and you will notice that it reaches higher accuracies than the previous models you've built."
            ],
            "metadata": {
                "id": "iLJu8HEvPG0L",
                "azdata_cell_guid": "75e2205a-7967-4736-8d3e-247c3c339c42"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "NUM_EPOCHS = 10\n",
                "\n",
                "# Train the model\n",
                "history = model.fit(train_dataset, epochs=NUM_EPOCHS, validation_data=test_dataset)"
            ],
            "metadata": {
                "id": "7mlgzaRDMtF6",
                "azdata_cell_guid": "dd37a579-bcb1-4838-86c3-769badce6703",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Plot utility\n",
                "def plot_graphs(history, string):\n",
                "  plt.plot(history.history[string])\n",
                "  plt.plot(history.history['val_'+string])\n",
                "  plt.xlabel(\"Epochs\")\n",
                "  plt.ylabel(string)\n",
                "  plt.legend([string, 'val_'+string])\n",
                "  plt.show()\n",
                "\n",
                "# Plot the accuracy and results \n",
                "plot_graphs(history, \"accuracy\")\n",
                "plot_graphs(history, \"loss\")"
            ],
            "metadata": {
                "id": "Mp1Z7P9pYRSK",
                "azdata_cell_guid": "b0c68b82-23af-4e14-9599-826691a430b3",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Wrap Up\n",
                "\n",
                "In this lab, you explored another model architecture you can use for text classification. In the next lessons, you will revisit full word encoding of the IMDB reviews and compare which model works best when the data is prepared that way."
            ],
            "metadata": {
                "id": "0rD7ZS84PlUp",
                "azdata_cell_guid": "1a20123f-6dfe-49c9-a71e-f2ef7ab98325"
            }
        }
    ]
}